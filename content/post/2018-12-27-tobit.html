---
title: "Tobit"
author: "Soonhong Cho"
output: html_document
---



<div id="censored-data" class="section level1">
<h1>1. Censored Data</h1>
<p><em>censored data</em>를 다루는 하나의 방법인 <em>tobit regression</em>을 이해해보자. 데이터가 <em>censoring</em>되었다는 것은, 특히 연속형 종속변수가 특정한 기준점 이상/이하의 값을 취할 수 없는 경우를 의미한다. 가령, 미국의 Social Security Administration에 기록된 소득 데이터는 (2006년 기준) $117,000 이하의 소득은 정확히 기록하고 있지만, $117,000 이상의 소득은 $117,000 으로 기록된다 (사회보장세의 최고 과세구간의 상한이 $117,000이기 때문). 이 경우 소득 변수는 $117,000 이상의 값을 취할 수 없다.\ <em>Tobit model</em>은 종속변수가 이렇게 <em>censored</em>되어있을 때 활용할 수 있다. Tobit model은 다음과 같은 간단한 선형 회귀 모형에서 시작한다. <span class="math display">\[y_i^* ~\sim~ N(\mu_i,\sigma^2)\]</span> and <span class="math display">\[\mu_i = \beta_0 + \sum_{k=1}^{K} \beta_k x_{ik}.\]</span> 위의 식에서 <span class="math inline">\(y_i^*\)</span>는 일종의 잠재변수(latent variable)이며 실제로 관측되지는 않는다. 실제 관측한, <em>censored</em> 종속변수는 특정 상수값 이상 혹은 이하에서 정확히 관측되지 않고, 그 상수값보다 크거나 작다는 사실만 알 수 있다. 가령, <span class="math inline">\(c\)</span>라는 알려진 상수보다 큰 값의 경우 (<span class="math inline">\(y^*_i &gt; c\)</span>) 우리는 오직 그 값이 <span class="math inline">\(c\)</span>보다 크다는 사실(<span class="math inline">\(y^*_i &gt; c\)</span>)만 알 수 있을 뿐, 정확한 값은 알 수 없다. 즉, 관측된 종속변수는 다음과 같이 표현된다. <span class="math display">\[y_i = \begin{cases}  y^*_i &amp; \mathrm{if}~y^*_i &lt; c\\
                       c    &amp; \mathrm{otherwise}\end{cases}.\]</span></p>
</div>
<div id="tobit-mle" class="section level1">
<h1>2. Tobit &amp; MLE</h1>
<div id="ordinary-least-square-estimator" class="section level2">
<h2>2.1. Ordinary Least Square estimator</h2>
<p>가상의 데이터를 만들어서 분석해보면, <em>censored data</em>의 특성에 대해 더 잘 이해해볼 수 있을 것이다. 연구자가 소득에 대한 교육의 효과를 연구한다고 하자. 예를 쉽게 하기 위해, 다른 모든 통제변수는 제외하고 오로지 교육변수만 설명변수로 하는 선형모형을 가정하자. 교육을 처리집단과 통제집단에 랜덤으로 배정하는 실험을 하고, 각 개인의 현재 소득에 대한 자료를 갖고 있다. 즉 종속변수는 현재 소득, 유일한 독립변수는 교육 여부(dummy variable)이다. \ 먼저, 가상의 <em>censored data</em>를 만들어내는 <strong>cendata</strong>라는 함수를 만든다. argument로 <span class="math inline">\(c\)</span>를 포함시켜, 특정 <span class="math inline">\(c\)</span>를 지정하면 그 이상의 값들은 <span class="math inline">\(c\)</span>로 censoring되도록 만들어준다. 이 함수에서는 <em>xdist</em>라는 argument를 만들어서 독립변수가 어떠한 분포든 취할 수 있도록 허용한다.</p>
<pre class="r"><code>set.seed(2018)

cendata &lt;- function(n=1000, beta0=10, beta1=1, sigma2=0.7, c=c, x=NULL, xdist=rbinom) {
  if (is.null(x)) {
    x &lt;- xdist(n, size=1, p=0.5)
    x &lt;- x/sd(x) # standardize to have sd of 1
  }
  eps &lt;- rnorm(n, mean=0, sd=sqrt(sigma2)) # Transform eps to have std dev of sigma
  y.star &lt;- beta0 + beta1*x + eps
  y &lt;- ifelse(y.star&lt;c, y.star, c) # right-censored at c
  return( data.frame(y=y, x=x, eps=eps) )
}</code></pre>
<p>위의 함수가 제대로 작동하는지 실행해보고, 이 데이터를 OLS로 분석했을 때 어떤 문제가 생기는지 확인해보자.. <span class="math inline">\(y\)</span>는 소득의 자연로그, <span class="math inline">\(x\)</span>는 교육여부이고, 교육을 받을 확률은 50%라고 가정하자 (p=0.5). 히스토그램을 그려 대략적 분포를 살펴보면, <span class="math inline">\(c=12\)</span>에서 데이터가 쌓이는 것(stack)을 확인할 수 있다. (실제 2006년 미국의 Social Security Administration의 소득자료와 분포가 비슷해지도록 모수값을 설정하였다.) 또한 교육의 효과 (<span class="math inline">\(\beta_1\)</span>)가 과소추정되었음을 확인할 수 있다 (우리가 설정한 실제 <span class="math inline">\(\beta_1\)</span>.은 1이었다.) 만약 더 많은 양의 데이터가 <em>censoring</em>된다면 모수의 추정치는 더 편향될 것이다. 아래의 <strong>dta.larger</strong>는 그러한 데이터를 만든 것이고, bias가 더 커졌음을 알 수 있다.</p>
<pre class="r"><code>## try data
n &lt;- 1000
x &lt;- rbinom(n=n, size=1, p=0.5) # x is a treatment dummy variable here (education)
dta &lt;- cendata(n=1000,x=x,c=12) # censoring point (c) = 12
hist(exp(dta$y), breaks = 50) # Suppose that y is ln(income). it seems like income distribution</code></pre>
<p><img src="/post/2018-12-27-tobit_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>summary(lm(y~x, data=dta))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = dta)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.25771 -0.54984  0.02499  0.61146  2.01819 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  9.98181    0.03608  276.63   &lt;2e-16 ***
## x            0.98524    0.05093   19.35   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8052 on 998 degrees of freedom
## Multiple R-squared:  0.2727, Adjusted R-squared:  0.272 
## F-statistic: 374.3 on 1 and 998 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>dta.larger &lt;- cendata(n=1000,x=x,c=11)
hist(exp(dta.larger$y), breaks = 50)</code></pre>
<p><img src="/post/2018-12-27-tobit_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>summary(lm(y~x, data=dta.larger))$coef</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 9.9201670 0.02836077 349.78479 0.000000e+00
## x           0.7353034 0.04002821  18.36963 3.791024e-65</code></pre>
<pre class="r"><code># lowing c -&gt; more censoring occurs -&gt; more biased estimates</code></pre>
</div>
<div id="tobit" class="section level2">
<h2>2.2. Tobit</h2>
<p>OLS estimator는 <em>censored data</em>에 적합하지 않음을 (“biased estimator”) 확인했으니, 이번에는 명시적으로 censoring을 고려하는 최대우도추정자(MLE, Maximum Likelihood Estimator)를 만들어보자. ML은 관측된 데이터를 토대로, 우리가 알고자 하는 모수를 추정하는(실제 갖고있는 데이터가 관측될 가능성이 가장 높은 모수의 집합을 찾아가는) 방법이다. 보통 데이터의 결합확률(joint probability)을 모수값들의 함수(이를 우도함수, likelihod function이라고 한다)로 놓고, 그 함수의 값을 최대로 만드는 모수값들을 찾는다. \ 우리의 예에서 censored data의 <span class="math inline">\(i\)</span>번째 관측자료의 우도함수는 다음과 같이 표현된다. <span class="math display">\[ L_i(\beta_0,\beta_1) = \begin{cases} f(y_i;\mu_i,\sigma^2) &amp; y_i &lt; c\\
                             1-F(c;\mu_i,\sigma^2) &amp; y_i = c\\ \end{cases}\]</span> 여기서 <span class="math inline">\(f\)</span>는 정규분포의 PDF, <span class="math inline">\(F\)</span>는 정규분포의 CDF, <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_i\)</span>이다. 왜 우도함수를 이렇게 만드는 것일까? 우선 데이터의 각 관측치(obseravtion)는 서로 독립적이라고 가정하기 때문에, 전체 우도함수는 각 관측치의 우도함수에 대한 기여(likelihood contribution)를 모두 곱한 것이다. 위의 식은 <span class="math inline">\(i\)</span>번째 관측치가 전체 우도함수에 기여하는 정도를 표현한 식이다. 만약 해당 값이 censoring point인 <span class="math inline">\(c\)</span>보다 작으면 그 값이 관측될 가능도는 정규분포의 확률밀도함수로 정의될 수 있다 (소득은 정규분포를 따른다고 가정하고, censoring되지 않는 정도의 소득은 정규분포에서 나왔다고 보는 것). 그러나 cesoring point인 <span class="math inline">\(c\)</span>보다 더 큰 소득의 경우, 우리는 정확히 그 사람의 소득이 얼마인지 알 수 없다. 우리가 아는 것은 오직 이 값이 <span class="math inline">\(c\)</span>보다는 크거나 같다는 사실 뿐이다. 이러한 관측값들은 제한된 정보만을 우리에게 제공하는 것이다. 그리고 그 제한된 정보를 반영하여, censoring된 관측값들의 우도함수에 대한 기여는 <span class="math inline">\(c\)</span>보다 큰 값이 나올 확률(<span class="math inline">\(1-F(c;\mu_i,\sigma^2)\)</span>)로 계산한다. <span class="math inline">\(L_i(\beta_0,\beta_1)\)</span>를 모두 곱한 전체 우도함수는 다음과 같이 표현된다. <span class="math display">\[ L(\beta_0,\beta_1)= \underbrace{\prod\limits_{y_i&lt;c}{f(y_i)}}_{uncensored} \times \underbrace{\prod\limits_{y_i=c}{(1-F(c))}}_{censored}\]</span></p>
<p>\ ML 방법은 보통 계산의 편의성을 위하여 우도함수에 로그를 취한 log-likelihood function을 최대화하는 모수를 찾는다. <span class="math inline">\(\theta=(\beta_0, \beta_1, \sigma)\)</span> 세 모수를 포함하는 log-likelihood function은 다음과 같다.</p>
<pre class="r"><code>tobit.ll &lt;- function(theta,y,x,c) {
  b0 &lt;- theta[1]
  b1 &lt;- theta[2]
  sigma &lt;- exp(theta[3]) # reparameterize because sigma&gt;0
  xb &lt;- b0 + b1*x
  cen &lt;- sum(pnorm(y[y==c], mean=xb[y==c], sd=sigma, log=T, lower.tail=F))
         # &quot;lower.tail=F&quot;: it calculates 1-pnorm()
  uncen &lt;- sum(dnorm(y[y&lt;c], mean=xb[y&lt;c], sd=sigma, log=T))
  return ( -(cen+uncen) ) # make it &quot;minimization problem&quot; for minimizer optim
}</code></pre>
<p>세 모수 중 우리가 관심있는 <span class="math inline">\(\beta_1\)</span>의 값의 변화에 따라 log-likelihood 함수값이 어떻게 달라지는지 그래프를 통해 살펴보자 (<span class="math inline">\(\beta_0\)</span>와 <span class="math inline">\(\sigma\)</span>의 값은 실제 값에 고정시켰다). 로그우도함수를 최대화하는 값이 <span class="math inline">\(\beta_1\)</span>의 MLE가 된다.</p>
<pre class="r"><code>library(ggplot2)
suppressPackageStartupMessages(library(tidyverse))

b1pts &lt;- seq(-3,3,length=1000)
llpts &lt;- sapply(b1pts, 
                function(m) 
                  -(tobit.ll(c(10,m,log(sqrt(0.7))), y=dta$y, x=dta$x, c=12)))
                  # fix b0 &amp; sig2 at their true values and apply *(-1) to see orginal ll fun.
b1pts[which.max(llpts)] # manually search for mle of beta_1</code></pre>
<pre><code>## [1] 1.012012</code></pre>
<pre class="r"><code>ggplot(tibble(b1pts,llpts), aes(x=b1pts, y=llpts)) +
  geom_line() +
  ylab(&quot;Log-likelihood&quot;) + 
  xlab(&quot;b1&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2018-12-27-tobit_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>다음으로, 먼저 패키지를 활용하지 않고 위에서 만든 <code>tobit.ll</code> 함수를 활용하여 <em>tobit regression</em>을 실행시킬 수 있는 함수를 직접 만들어보자. 데이터로 <span class="math inline">\((x, y)\)</span>를 받고, <span class="math inline">\(c\)</span>로 censoring point를 지정해준다. maximization을 위해 <code>optim</code>을 활용한다.</p>
<pre class="r"><code>mytobit &lt;- function(y, x, c){
  tobit.ll &lt;- function(theta,y,x,c) {
    b0 &lt;- theta[1]
    b1 &lt;- theta[2]
    sigma &lt;- exp(theta[3]) # reparameterize becase sigma&gt;0
    xb &lt;- b0 + b1*x
    cen &lt;- sum(pnorm(y[y==c], mean=xb[y==c], sd=sigma, log=T, lower.tail=F))
    uncen &lt;- sum(dnorm(y[y&lt;c], mean=xb[y&lt;c], sd=sigma, log=T))
    return ( -(cen+uncen) ) # make it &quot;minimization problem&quot; for minimizer optim
  }
  
  ml.res &lt;- optim( c(10,1,1), tobit.ll, y=y, x=x, c=c, hessian=TRUE)
  # startvalues: theta = c(10,1,1)

  est &lt;- ml.res$par
  se &lt;- sqrt(diag(solve(ml.res$hessian)))
  res &lt;- as.matrix(cbind(est,se))
  colnames(res) &lt;- c(&quot;Estimate&quot;, &quot;Std. Error&quot;)
  rownames(res) &lt;- c(&quot;b0&quot;, &quot;b1&quot;, &quot;log(sigma)&quot;)

  return(res)
}</code></pre>
<p>제대로 작동하는지 확인하기 위해 <code>AER</code> 패키지의 <code>tobit</code>함수의 결과와 비교해보자.</p>
<pre class="r"><code>suppressPackageStartupMessages(library(AER))
mytobit.aer &lt;- function(y, x, c){
  coef(summary(tobit(y~x,data=dta, left = -Inf, right = max(dta$y))))[1:3, 1:2]  
}

mytobit(y=dta$y, x=dta$x, c=12)</code></pre>
<pre><code>##              Estimate Std. Error
## b0          9.9845960 0.03798495
## b1          1.0291264 0.05389334
## log(sigma) -0.1657433 0.02342421</code></pre>
<pre class="r"><code>mytobit.aer(y=dta$y, x=dta$x, c=12)</code></pre>
<pre><code>##               Estimate Std. Error
## (Intercept)  9.9846496 0.03798767
## x            1.0290960 0.05389717
## Log(scale)  -0.1656719 0.02342589</code></pre>
</div>
<div id="sampling-distribution-and-consistency-of-estimator" class="section level2">
<h2>2.3 Sampling Distribution and Consistency of Estimator</h2>
<p>이 모델은 <span class="math inline">\(\beta_1\)</span>의 일치추정량(consistent estimates)을 제공해주는가? 간단한 Monte Carlo Simulation을 통해 직접 <span class="math inline">\(\hat{\beta_1}\)</span>의 표본분포(sampling distribution)을 살펴보고, sample size가 커짐에 따라 보다 정확한 추정을 할 수 있는지 확인해보자.</p>
<pre class="r"><code>simulate &lt;- function(sims=1000,n=1000,c=c) {
  results &lt;- matrix(NA,sims,4) # container
  colnames(results) &lt;- c(&quot;b0&quot;,&quot;b1&quot;,&quot;se.b0&quot;,&quot;se.b1&quot;)
  x &lt;- rbinom(n, size=1, p=0.5) # generate random x
  for (j in 1:sims) {
    dta &lt;- cendata(n=n, x=x, c=c) # pass c value for the simulation
    res &lt;- mytobit(y=dta$y, x=dta$x, c=c)
    results[j,] &lt;- c(res[1,1], res[2,1], res[1,2], res[2,2])
  }
  results
}</code></pre>
<pre class="r"><code>sims=1000 # sims: # of sampling / n: # of obs. for each sampling
sim.s &lt;- simulate(sims=sims, n=10, c=12) # small N
sim.l &lt;- simulate(sims=sims, n=1000, c=12) # large N

## Plot sampling distributions of beta1
compare &lt;- as.data.frame(cbind(sim.s[,2], sim.l[,2])) %&gt;%
        rename(&quot;10&quot;=V1, &quot;1000&quot;=V2) %&gt;%
        gather(N, beta1)

compare_mean &lt;- compare %&gt;%
        group_by(N) %&gt;%
        summarise(Mean = mean(beta1), SD = sd(beta1))
compare_mean # means for sim.s &amp; sim.l</code></pre>
<pre><code>## # A tibble: 2 x 3
##   N      Mean     SD
##   &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1 10     1.09 0.928 
## 2 1000   1.00 0.0543</code></pre>
<pre class="r"><code>ggplot(data=compare, aes(x=beta1, fill=N)) +
      ggtitle(&quot;Sampling Distribution of beta1&quot;) +
      theme(plot.title = element_text(hjust=0.5, size=20)) +
      geom_density(alpha=0.5) +
      geom_vline(data = compare_mean, linetype=&quot;dashed&quot;,
                 aes(xintercept=Mean, color=N), size=1) +
      xlim(-1, 3) +
      facet_grid(N ~ .)</code></pre>
<pre><code>## Warning: Removed 18 rows containing non-finite values (stat_density).</code></pre>
<p><img src="/post/2018-12-27-tobit_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="degree-of-censoring" class="section level2">
<h2>2.4 Degree of Censoring</h2>
<p>Censoring되는 데이터의 양이 많아지면, 즉 데이터가 주는 정보가 점점 더 적어지면 추정의 불확실성은 커져야 할 것이다. Tobit model은 그러한 특성을 가지고 있는지, 다시 한 번 Monte Carlo 시뮬레이션을 통해서 확인해보자. \ 우리의 예는 특정 값(<span class="math inline">\(c\)</span>) 이상의 경우 censor되도록 설정하고 있으므로, <span class="math inline">\(c\)</span>의 값을 점점 더 작게 할수록 censoring되는 데이터의 양은 많아질 것이다. <span class="math inline">\(c\)</span>의 변화에 따라 <span class="math inline">\(\beta_1\)</span>의 표준오차가 어떤 행태를 보이는지 시뮬레이션을 통해 알아보자.</p>
</div>
</div>
